{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 다운로드 링크: https://www.kaggle.com/yelp-dataset/yelp-dataset/data?select=yelp_academic_dataset_review.json \n",
    "                        #yelp_academic_dataset_review.json(5.89 GB) → 이 파일 다운로드 \n",
    "#추가 참고자료: https://wikidocs.net/book/2155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Text Data: Flattening, Filtering, and Chunking\n",
    "텍스트 데이터의 기본적인 feature engineering을 다루는 챕터<br>\n",
    "텍스트 데이터를 수치화, 전처리, 다양한 단위(word, <i>n</i>-gram, phrase)로 분리하는 방법에 대해서 다룸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag-of-X: Turning Natural Text into Flat Vectors \n",
    "토큰의 출현 빈도를 통해 텍스트 데이터를 수치화하는 방법<br>\n",
    "토큰(token): 텍스트 처리에 있어 유용하고 의미있는 단위 (보통 단어)\n",
    "\n",
    "### 1. Bag-of-Words\n",
    "- 텍스트 데이터를 각 단어의 카운트로 이루어진 flat vector로 변환 (기존 텍스트의 문장구조가 사라지기 때문에 'flat')\n",
    "- feautre는 단어, feature vector은 그 단어의 카운트를 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/figure3-1.png\" width=\"500\" height=\"500\">\n",
    "<center>(Feature Engineering for Machine Learning by Alice Zheng and Amanda Casari (O’Reilly))</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 시각화\n",
    "- data vector를 feature space에 나타낼 수 있고, 반대로 feature vector를 data space에 나타낼 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/figure3-2.png\" width=\"500\" height=\"500\">\n",
    "<center>data vector를 feature space에 나타냄</center>\n",
    "<img src=\"img/figure3-3.png\" width=\"500\" height=\"500\">\n",
    "<center>feature vecter를 data space에 나타냄</center>\n",
    "<center>(Feature Engineering for Machine Learning by Alice Zheng and Amanda Casari (O’Reilly))</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 문제점\n",
    "- 단어 간 순서에 대한 정보가 없음\n",
    "- 단어 간 위계에 대한 정보를 표현할 수 없음 (예: 동물, 강아지, 고양이가 모두 동일한 위계를 가짐)\n",
    "- semantic meaning을 없애버리게 됨 (예: \"not bad\"이 긍정의 의미를 가지고 있지만, 단일 단어로 나누게 되면 이런 의미를 상실하게 됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bag-of-<i>n</i>-Grams \n",
    "- bag-of-words의 개념을 확장시킨 방법\n",
    "- <i>n</i>-gram: a sequence of <i>n</i> tokens\n",
    "- <i>n</i>-grams는 기존 단어 순서에 대한 정보를 어느정도 가지고 있음\n",
    "\n",
    "##### 문제점\n",
    "- 보통 <i>n</i>-grams은 단일 단어보다 그 갯수가 많음. 그로 인해 <i>n</i>-grams를 모델에 이용할 때 더 많은 비용(storage cost, computation cost)이 들고, 데이터들이 더 sparse 해짐(데이터는 그대로인데 feature dimension이 커지기 때문) \n",
    "- 따라서 <i>n</i>이 증가한다고 해서 모델의 정확도가 항상 향상되는 것이 아님. bigram 또는 trigram을 가장 많이 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i>n</i>-grams 구하기 \n",
    "- CountVectorizer transformer 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-MhfebM0QIsKt87iDN-FNw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04-15 05:21:16</td>\n",
       "      <td>0</td>\n",
       "      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "      <td>5</td>\n",
       "      <td>OwjRMXRC0KyPrIlcjaXeFQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lbrU8StCq3yDfr-QMnGrmQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-12-07 03:16:52</td>\n",
       "      <td>1</td>\n",
       "      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "      <td>1</td>\n",
       "      <td>nIJD_7ZXHq-FX8byPMOkMQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HQl28KMwrEKHqhFrrDqVNQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-05 03:18:11</td>\n",
       "      <td>0</td>\n",
       "      <td>LG2ZaYiOgpr2DK_90pYjNw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "      <td>1</td>\n",
       "      <td>V34qejxNsCbcgD8C0HVk-Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5JxlZaqCnk1MnbgRirs40Q</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-05-27 05:30:52</td>\n",
       "      <td>0</td>\n",
       "      <td>i6g_oA9Yf9Y31qt0wibXpw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "      <td>0</td>\n",
       "      <td>ofKDkJKXSKZXu5xJNGiiBQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IS4cv902ykd8wj1TR0N3-A</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-14 21:56:57</td>\n",
       "      <td>0</td>\n",
       "      <td>6TdNDKywdbjoTkizeMce8A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>UgMW8bLE0QMJDCkQ1Ax5Mg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  -MhfebM0QIsKt87iDN-FNw     0  2015-04-15 05:21:16      0   \n",
       "1  lbrU8StCq3yDfr-QMnGrmQ     0  2013-12-07 03:16:52      1   \n",
       "2  HQl28KMwrEKHqhFrrDqVNQ     0  2015-12-05 03:18:11      0   \n",
       "3  5JxlZaqCnk1MnbgRirs40Q     0  2011-05-27 05:30:52      0   \n",
       "4  IS4cv902ykd8wj1TR0N3-A     0  2017-01-14 21:56:57      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  xQY8N_XvtGbearJ5X4QryQ    2.0   \n",
       "1  UmFMZ8PyXZTY2QcwzsfQYA    1.0   \n",
       "2  LG2ZaYiOgpr2DK_90pYjNw    5.0   \n",
       "3  i6g_oA9Yf9Y31qt0wibXpw    1.0   \n",
       "4  6TdNDKywdbjoTkizeMce8A    4.0   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  As someone who has worked with many museums, I...       5   \n",
       "1  I am actually horrified this place is still in...       1   \n",
       "2  I love Deagan's. I do. I really do. The atmosp...       1   \n",
       "3  Dismal, lukewarm, defrosted-tasting \"TexMex\" g...       0   \n",
       "4  Oh happy day, finally have a Canes near my cas...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  OwjRMXRC0KyPrIlcjaXeFQ  \n",
       "1  nIJD_7ZXHq-FX8byPMOkMQ  \n",
       "2  V34qejxNsCbcgD8C0HVk-Q  \n",
       "3  ofKDkJKXSKZXu5xJNGiiBQ  \n",
       "4  UgMW8bLE0QMJDCkQ1Ax5Mg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/alicezheng/feature-engineering-book/blob/master/03.01_Count_words.ipynb 참고 \n",
    "\n",
    "#10,000개의 리뷰 데이터를 불러와서 데이터프레임 만들기\n",
    "f = open('C:\\\\Users\\\\이인주\\\\Desktop\\\\2020 summer study\\\\feature_engineering\\\\data\\\\yelp_academic_dataset_review.json', 'r', encoding='UTF-8') \n",
    "        #데이터 경로 입력\n",
    "js = [] #리뷰 데이터를 추가할 리스트 생성 \n",
    "\n",
    "for i in range(10000):\n",
    "    js.append(json.loads(f.readline())) #json.loads(): json 형식의 문자열을 파이썬 사전의 형태로 변환\n",
    "                                        #readline(): 한줄씩 읽어들임 \n",
    "f.close()\n",
    "\n",
    "review_df = pd.DataFrame(js) #리스트를 데이터프레임으로 변환\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As someone who has worked with many museums, I was eager to visit this gallery on my most recent trip to Las Vegas. When I saw they would be showing infamous eggs of the House of Faberge from the Virginia Museum of Fine Arts (VMFA), I knew I had to go!\\n\\nTucked away near the gelateria and the garden, the Gallery is pretty much hidden from view. It\\'s what real estate agents would call \"cozy\" or \"charming\" - basically any euphemism for small.\\n\\nThat being said, you can still see wonderful art at a gallery of any size, so why the two *s you ask? Let me tell you:\\n\\n* pricing for this, while relatively inexpensive for a Las Vegas attraction, is completely over the top. For the space and the amount of art you can fit in there, it is a bit much.\\n* it\\'s not kid friendly at all. Seriously, don\\'t bring them.\\n* the security is not trained properly for the show. When the curating and design teams collaborate for exhibitions, there is a definite flow. That means visitors should view the art in a certain sequence, whether it be by historical period or cultural significance (this is how audio guides are usually developed). When I arrived in the gallery I could not tell where to start, and security was certainly not helpful. I was told to \"just look around\" and \"do whatever.\" \\n\\nAt such a *fine* institution, I find the lack of knowledge and respect for the art appalling.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#리뷰 데이터 예시 \n",
    "review_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/alicezheng/feature-engineering-book/blob/master/03.01_Count_words.ipynb 참고\n",
    "\n",
    "#텍스트를 unigram, bigram, trigram으로 나누어주는 각 feature transformer 설정 \n",
    "unigram_converter = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b') #token_pattern='(?u)\\\\b\\\\w+\\\\b': 정규식 표현. 공백 사이에 있는 2글자 이상의 문자열을 찾아 토큰으로 분해 \n",
    "bigram_converter = CountVectorizer(ngram_range=(2,2), token_pattern='(?u)\\\\b\\\\w+\\\\b') #ngram_range(min_n, max_n): n-grams의 n의 최소값과 최대값 \n",
    "trigram_converter = CountVectorizer(ngram_range=(3,3), token_pattern='(?u)\\\\b\\\\w+\\\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advised',\n",
       " 'adviser',\n",
       " 'advises',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advisors',\n",
       " 'advocate',\n",
       " 'aerobic',\n",
       " 'aerobics',\n",
       " 'aesthetic',\n",
       " 'aesthetically',\n",
       " 'aesthetician',\n",
       " 'aesthetics',\n",
       " 'af',\n",
       " 'afaik',\n",
       " 'afb',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/alicezheng/feature-engineering-book/blob/master/03.01_Count_words.ipynb 참고\n",
    "\n",
    "unigram = unigram_converter.fit(review_df['text']) #앞에서 만든 feature transformer를 통해 텍스트를 unigram으로 나누기\n",
    "one = unigram.get_feature_names() #get_feature_names(): feature의 목록을 볼 수 있음 \n",
    "one[1000:1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 pounds',\n",
       " '2 pours',\n",
       " '2 price',\n",
       " '2 priced',\n",
       " '2 prime',\n",
       " '2 probably',\n",
       " '2 puke',\n",
       " '2 pumps',\n",
       " '2 queen',\n",
       " '2 queens',\n",
       " '2 quotes',\n",
       " '2 r',\n",
       " '2 race',\n",
       " '2 races',\n",
       " '2 read',\n",
       " '2 reasons',\n",
       " '2 recessed',\n",
       " '2 refills',\n",
       " '2 registers',\n",
       " '2 regular']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/alicezheng/feature-engineering-book/blob/master/03.01_Count_words.ipynb 참고\n",
    "\n",
    "bigram = bigram_converter.fit(review_df['text']) \n",
    "two = bigram.get_feature_names() \n",
    "two[2000:2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13 seconds to',\n",
       " '13 that included',\n",
       " '13 the 12oz',\n",
       " '13 through monday',\n",
       " '13 tip the',\n",
       " '13 tip to',\n",
       " '13 unfortunately they',\n",
       " '13 was definitely',\n",
       " '13 were scratched',\n",
       " '13 when making']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/alicezheng/feature-engineering-book/blob/master/03.01_Count_words.ipynb 참고\n",
    "\n",
    "trigram = trigram_converter.fit(review_df['text']) \n",
    "third = trigram.get_feature_names() \n",
    "third[2000:2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtering for Cleaner Features \n",
    "텍스트 데이터를 전처리하는 방법에 대해 다룸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stopwords(불용어)\n",
    "- 분석에 있어 크게 도움이 되지 않는 토큰을 제거 \n",
    "- 파이썬 NLP 패키지 중 하나인 'NLTK'에서 다양한 언어의 불용어 목록을 제공함 \n",
    "- English stopword list: a, about, above, am, an, been, didn't ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Frequency-Based Filtering \n",
    "- 토큰의 <b>출현 빈도</b>에 기반하여 데이터를 전처리 \n",
    "\n",
    "#### (1) Frequent words \n",
    "- 빈도수가 큰 토큰을 제거\n",
    "- 출현 빈도수를 이용하면 general-purpose stopword 뿐만 아니라 corpus-specific common word까지 필터링할 수 있음<br>\n",
    "예시: New Youk Times 기사 텍스트에서 'New York Times'라는 구(phrase)와 각 단어의 출현 빈도수는 큼. 그러나 해당 구와 단어가 텍스트 데이터에서 중요한 의미를 가지고 있는 것은 아님\n",
    "- 출현 빈도수에 기반하여 필터링 할 때 빈번함 정도의 cutoff를 정하는 것이 어려움\n",
    "- 보통 frequency-based filtering과 stopword list를 함께 사용하여 전처리 함\n",
    "\n",
    "#### (2) Rare words \n",
    "- 빈도수가 작은(1-2회) 토큰들을 제거\n",
    "- 보통 rare word는 predictor로서 unreliable하고 많은 computational 비용을 만들어내기 때문에 제거함\n",
    "- rare word를 벡터 리스트에서 지워버리는 방법과 'garbage'라는 bin을 통해 rare word의 갯수를 따로 세는 방법이 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/figure3-4.png\" width=\"500\" height=\"500\">\n",
    "<center>(Feature Engineering for Machine Learning by Alice Zheng and Amanda Casari (O’Reilly))</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming and Lemmatization\n",
    "텍스트 데이터의 단어들을 단순하게 파싱(parsing)하면 한 단어의 다양한 형태가 모두 서로 다른 독립적인 단어로 카운트되는 문제점이 발생<br>\n",
    "(예: '꽃', '꽃들'이 결국 같은 단어임에도 서로 다른 독립적인 단어로 카운트)\n",
    "<br><br>\n",
    "이를 해결하기 위해 <b>Stemming(어간추출)</b> 또는 <b>Lemmatization(표제어추출)</b>을 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Stemming (어간추출) \n",
    "- 어간(stem)을 추출하는 작업 (어간: 단어의 핵심적인 의미를 담고 있는 부분. (예: 꽃들 → 꽃))\n",
    "- 형태학적 분석을 단순화함. 정해진 규칙을 보고 단어의 어미를 자르는 어림짐작의 작업 \n",
    "- 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과가 사전에 존재하지 않은 단어일 수 있음 \n",
    "- 품사정보가 보존되지 않음 (POS(Part-Of-Speech) 태그를 고려하지 않음)\n",
    "- Porter stemmer: 영어에 사용할 수 있는 stemming tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'someone', 'who', 'has', 'worked', 'with', 'many', 'museums', ',', 'I', 'was', 'eager', 'to', 'visit', 'this', 'gallery', 'on', 'my', 'most', 'recent', 'trip', 'to', 'Las', 'Vegas']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"As someone who has worked with many museums, I was eager to visit this gallery on my most recent trip to Las Vegas\"\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer() #.TreebankWordTokenizer(): 문법규칙에 기반하여 텍스트의 토큰을 구분\n",
    "tokens = tokenizer.tokenize(text)#텍스트 데이터를 토큰화함\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'someon', 'who', 'ha', 'work', 'with', 'mani', 'museum', ',', 'I', 'wa', 'eager', 'to', 'visit', 'thi', 'galleri', 'on', 'my', 'most', 'recent', 'trip', 'to', 'la', 'vega']\n"
     ]
    }
   ],
   "source": [
    "L = []\n",
    "stemmer = nltk.stem.PorterStemmer() #stemmer 설정\n",
    "\n",
    "for token in tokens:\n",
    "    stem = stemmer.stem(token)\n",
    "    L.append(stem)\n",
    "    \n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Lemmatization (표제어추출)\n",
    "- 표제어(Lemma)를 추출하는 작업 (표제어: 기본 사전형 단어)\n",
    "- 단어의 접사(affix)와 어간(stem)을 분리하는 형태학적 파싱을 통해 표제어를 추출\n",
    "- 품사정보가 보존(POS 태그를 보존)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\이인주\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') #.WordNetLemmatizer() 실행시 오류가 떴을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'someone', 'who', 'ha', 'worked', 'with', 'many', 'museum', ',', 'I', 'wa', 'eager', 'to', 'visit', 'this', 'gallery', 'on', 'my', 'most', 'recent', 'trip', 'to', 'Las', 'Vegas']\n"
     ]
    }
   ],
   "source": [
    "L = []\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer() #lemmatizer 설정\n",
    "\n",
    "for token in tokens:\n",
    "    lemma = lemmatizer.lemmatize(token)\n",
    "    L.append(lemma)\n",
    "    \n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표제어 추출기(lemmatizer)에 본래 단어의 품사 정보를 입력해줄 경우 더 정확하게 표제어를 출력할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Atoms of Meaning: From Words to <i>n</i>-Grams to Phrases \n",
    "텍스트 데이터를 word, <i>n</i>-grams, phrase의 형태로 분리하는 방법에 대하여 다룸<br>\n",
    "텍스트를 <i>n</i>-grams, phrase의 형태로 분리하면 텍스트 문장구조에 대한 정보를 얻을 수 있음 \n",
    "\n",
    "### 1. Parsing and Tokenization\n",
    "텍스트 데이터를 토큰의 형태로 분리하는 과정 \n",
    "##### Parsing\n",
    "문서가 순수 텍스트 이외의 다른 요소들을 포함하고 있을 때 사용<br>\n",
    "예: 다루고자 하는 텍스트 데이터가 이메일의 형태라면 From, To 등은 별개로 다루어야 함 \n",
    "##### Tokenization\n",
    "파싱을 통해 얻은 순수 텍스트 데이터를 토큰의 형태로 분리<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collocation Extracion for Phrase Detection \n",
    "우리는 텍스트를 읽을 때, 단어 하나하나를 이해하기 보단 구(phrase) 자체를 이해함으로써 텍스트 내용을 이해하는 경우가 많음<br>\n",
    "텍스트 데이터에서 개별적인 단어가 아닌 collocation을 추출해내는 방법을 다룸 \n",
    "\n",
    "##### Collocation\n",
    "- 특정 의미를 나타내기 위해 흔히 함께 쓰이는 2개 이상의 단어들의 결합 \n",
    "- collocation은 각 단어 의미의 합 이상의 의미를 지님 (예: strong tea > strong + tea )\n",
    "- 반드시 문장 안에서 연이어 위치할 필요는 없음 (예: Emma knocked on the door → knock door)\n",
    "- Bag-of-<i>n</i>-grams 방법으로는 텍스트로부터 collocation을 제대로 추출해내기 어려움 \n",
    "\n",
    "#### Collocation을 추출해내는 방법\n",
    "##### 1. Predefining \n",
    "- 자주 사용하는 collocation들을 사전에 정의해두는 방법 \n",
    "- 신조어 등을 바로 반영하지 못하는 문제점이 있음 \n",
    "- 따라서 통계적인 방법을 사용하는 것이 좋음 (Frequency-based methods, Hypothesis testing, Chunking and part-of-speech tagging)\n",
    "\n",
    "##### 2. Frequency-based methods \n",
    "- 빈번하게 나타난 <i>n</i>-grams을 이용하여 collocation을 추출\n",
    "- 문제점: 빈도수가 크다고 해서 반드시 의미가 있는 <i>n</i>-grams인 것은 아님 \n",
    "\n",
    "##### 3. Hypothesis testing for collocation extraction\n",
    "- 두 단어가 같이 사용되는 확률이 우연 수준보다 큰지 확인 \n",
    "- 대표적인 방법: likelihood ratio test\n",
    "\n",
    "##### likelihood ratio test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/figure3-5.png\" width=\"500\" height=\"500\">\n",
    "<img src=\"img/figure3-6.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- likelihood function L(Data; H): 해당 가설이 참일 때 데이터셋에서 단어 빈도수가 나타날 확률을 도출하는 함수   \n",
    "- 두 단어가 같이 사용되는 확률이 우연 수준의 확률보다 클 때 likelihood ratio 값(log λ)이 작아짐\n",
    "- likelihood ratio 값(log λ)이 작은 단어의 쌍을 feature로 선택 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Chunking and Part-Of-Speech(POS; 품사) tagging\n",
    "- 긴 collocation을 추출해내는 방법  \n",
    "- 품사 정보를 기반으로 sequences of tokens를 만들어냄  \n",
    "- 텍스트를 토큰화하는 과정에서 각 토큰의 품사를 태깅해놓고, 토큰의 주변을 확인하며 서로 연결되는 토큰들을 그룹핑\n",
    "- NLTK, spaCy, TextBlob, KoNLPY(한국어) 등의 파이썬 라이버리를 통해 각 토큰을 품사와 맵핑할 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/alicezheng/feature-engineering-book/blob/master/03.02_Chunking_and_POS_Tagging.ipynb 참고\n",
    "\n",
    "f = open('C:\\\\Users\\\\이인주\\\\Desktop\\\\2020 summer study\\\\feature_engineering\\\\data\\\\yelp_academic_dataset_review.json', 'r', encoding='UTF-8') #데이터 경로 입력 \n",
    "\n",
    "js = []\n",
    "for i in range(10):\n",
    "    js.append(json.loads(f.readline()))\n",
    "f.close()\n",
    "\n",
    "review_df = pd.DataFrame(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/alicezheng/feature-engineering-book/blob/master/03.02_Chunking_and_POS_Tagging.ipynb 참고\n",
    "\n",
    "nlp = spacy.load('en') #en(English) 모델을 불러옴\n",
    "                      #[E050] Can't find model 'en'. 에러가 뜰 경우 Anaconda Prompt를 관리자 권한으로 실행하여 'python -m spacy download en'를 설치 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (As, someone, who, has, worked, with, many, mu...\n",
       "1    (I, am, actually, horrified, this, place, is, ...\n",
       "2    (I, love, Deagan, 's, ., I, do, ., I, really, ...\n",
       "3    (Dismal, ,, lukewarm, ,, defrosted, -, tasting...\n",
       "4    (Oh, happy, day, ,, finally, have, a, Canes, n...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/alicezheng/feature-engineering-book/blob/master/03.02_Chunking_and_POS_Tagging.ipynb 참고\n",
    "\n",
    "doc_df = review_df['text'].apply(nlp) \n",
    "doc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As SCONJ IN\n",
      "someone PRON NN\n",
      "who PRON WP\n",
      "has AUX VBZ\n",
      "worked VERB VBN\n",
      "with ADP IN\n",
      "many ADJ JJ\n",
      "museums NOUN NNS\n",
      ", PUNCT ,\n",
      "I PRON PRP\n",
      "was AUX VBD\n",
      "eager ADJ JJ\n",
      "to PART TO\n",
      "visit VERB VB\n",
      "this DET DT\n",
      "gallery NOUN NN\n",
      "on ADP IN\n",
      "my DET PRP$\n",
      "most ADV RBS\n",
      "recent ADJ JJ\n",
      "trip NOUN NN\n",
      "to ADP IN\n",
      "Las PROPN NNP\n",
      "Vegas PROPN NNP\n",
      ". PUNCT .\n",
      "When ADV WRB\n",
      "I PRON PRP\n",
      "saw VERB VBD\n",
      "they PRON PRP\n",
      "would VERB MD\n",
      "be AUX VB\n",
      "showing VERB VBG\n",
      "infamous ADJ JJ\n",
      "eggs NOUN NNS\n",
      "of ADP IN\n",
      "the DET DT\n",
      "House PROPN NNP\n",
      "of ADP IN\n",
      "Faberge PROPN NNP\n",
      "from ADP IN\n",
      "the DET DT\n",
      "Virginia PROPN NNP\n",
      "Museum PROPN NNP\n",
      "of ADP IN\n",
      "Fine PROPN NNP\n",
      "Arts PROPN NNP\n",
      "( PUNCT -LRB-\n",
      "VMFA PROPN NNP\n",
      ") PUNCT -RRB-\n",
      ", PUNCT ,\n",
      "I PRON PRP\n",
      "knew VERB VBD\n",
      "I PRON PRP\n",
      "had AUX VBD\n",
      "to PART TO\n",
      "go VERB VB\n",
      "! PUNCT .\n",
      "\n",
      "\n",
      " SPACE _SP\n",
      "Tucked VERB VBN\n",
      "away ADV RB\n",
      "near SCONJ IN\n",
      "the DET DT\n",
      "gelateria PROPN NNP\n",
      "and CCONJ CC\n",
      "the DET DT\n",
      "garden NOUN NN\n",
      ", PUNCT ,\n",
      "the DET DT\n",
      "Gallery PROPN NNP\n",
      "is AUX VBZ\n",
      "pretty ADV RB\n",
      "much ADV RB\n",
      "hidden VERB VBN\n",
      "from ADP IN\n",
      "view NOUN NN\n",
      ". PUNCT .\n",
      "It PRON PRP\n",
      "'s AUX VBZ\n",
      "what PRON WP\n",
      "real ADJ JJ\n",
      "estate NOUN NN\n",
      "agents NOUN NNS\n",
      "would VERB MD\n",
      "call VERB VB\n",
      "\" PUNCT ``\n",
      "cozy ADJ JJ\n",
      "\" PUNCT ''\n",
      "or CCONJ CC\n",
      "\" PUNCT ``\n",
      "charming ADJ JJ\n",
      "\" PUNCT ''\n",
      "- PUNCT ,\n",
      "basically ADV RB\n",
      "any DET DT\n",
      "euphemism NOUN NN\n",
      "for ADP IN\n",
      "small ADJ JJ\n",
      ". PUNCT .\n",
      "\n",
      "\n",
      " SPACE _SP\n",
      "That DET DT\n",
      "being AUX VBG\n",
      "said VERB VBN\n",
      ", PUNCT ,\n",
      "you PRON PRP\n",
      "can VERB MD\n",
      "still ADV RB\n",
      "see VERB VB\n",
      "wonderful ADJ JJ\n",
      "art NOUN NN\n",
      "at ADP IN\n",
      "a DET DT\n",
      "gallery NOUN NN\n",
      "of ADP IN\n",
      "any DET DT\n",
      "size NOUN NN\n",
      ", PUNCT ,\n",
      "so ADV RB\n",
      "why ADV WRB\n",
      "the DET DT\n",
      "two NUM CD\n",
      "* PUNCT NFP\n",
      "s NOUN NN\n",
      "you PRON PRP\n",
      "ask VERB VBP\n",
      "? PUNCT .\n",
      "Let VERB VB\n",
      "me PRON PRP\n",
      "tell VERB VB\n",
      "you PRON PRP\n",
      ": PUNCT :\n",
      "\n",
      "\n",
      " SPACE _SP\n",
      "* PUNCT NFP\n",
      "pricing NOUN NN\n",
      "for ADP IN\n",
      "this DET DT\n",
      ", PUNCT ,\n",
      "while SCONJ IN\n",
      "relatively ADV RB\n",
      "inexpensive ADJ JJ\n",
      "for ADP IN\n",
      "a DET DT\n",
      "Las PROPN NNP\n",
      "Vegas PROPN NNP\n",
      "attraction NOUN NN\n",
      ", PUNCT ,\n",
      "is AUX VBZ\n",
      "completely ADV RB\n",
      "over ADP IN\n",
      "the DET DT\n",
      "top NOUN NN\n",
      ". PUNCT .\n",
      "For ADP IN\n",
      "the DET DT\n",
      "space NOUN NN\n",
      "and CCONJ CC\n",
      "the DET DT\n",
      "amount NOUN NN\n",
      "of ADP IN\n",
      "art NOUN NN\n",
      "you PRON PRP\n",
      "can VERB MD\n",
      "fit VERB VB\n",
      "in ADV RB\n",
      "there ADV RB\n",
      ", PUNCT ,\n",
      "it PRON PRP\n",
      "is AUX VBZ\n",
      "a DET DT\n",
      "bit NOUN NN\n",
      "much ADJ JJ\n",
      ". PUNCT .\n",
      "\n",
      " SPACE _SP\n",
      "* PUNCT NFP\n",
      "it PRON PRP\n",
      "'s AUX VBZ\n",
      "not PART RB\n",
      "kid NOUN NN\n",
      "friendly ADJ JJ\n",
      "at ADV RB\n",
      "all ADV RB\n",
      ". PUNCT .\n",
      "Seriously ADV RB\n",
      ", PUNCT ,\n",
      "do AUX VB\n",
      "n't PART RB\n",
      "bring VERB VB\n",
      "them PRON PRP\n",
      ". PUNCT .\n",
      "\n",
      " SPACE _SP\n",
      "* PUNCT NFP\n",
      "the DET DT\n",
      "security NOUN NN\n",
      "is AUX VBZ\n",
      "not PART RB\n",
      "trained VERB VBN\n",
      "properly ADV RB\n",
      "for ADP IN\n",
      "the DET DT\n",
      "show NOUN NN\n",
      ". PUNCT .\n",
      "When ADV WRB\n",
      "the DET DT\n",
      "curating VERB VBG\n",
      "and CCONJ CC\n",
      "design NOUN NN\n",
      "teams NOUN NNS\n",
      "collaborate VERB VBP\n",
      "for ADP IN\n",
      "exhibitions NOUN NNS\n",
      ", PUNCT ,\n",
      "there PRON EX\n",
      "is AUX VBZ\n",
      "a DET DT\n",
      "definite ADJ JJ\n",
      "flow NOUN NN\n",
      ". PUNCT .\n",
      "That DET DT\n",
      "means VERB VBZ\n",
      "visitors NOUN NNS\n",
      "should VERB MD\n",
      "view VERB VB\n",
      "the DET DT\n",
      "art NOUN NN\n",
      "in ADP IN\n",
      "a DET DT\n",
      "certain ADJ JJ\n",
      "sequence NOUN NN\n",
      ", PUNCT ,\n",
      "whether SCONJ IN\n",
      "it PRON PRP\n",
      "be AUX VB\n",
      "by ADP IN\n",
      "historical ADJ JJ\n",
      "period NOUN NN\n",
      "or CCONJ CC\n",
      "cultural ADJ JJ\n",
      "significance NOUN NN\n",
      "( PUNCT -LRB-\n",
      "this DET DT\n",
      "is AUX VBZ\n",
      "how ADV WRB\n",
      "audio ADJ JJ\n",
      "guides NOUN NNS\n",
      "are AUX VBP\n",
      "usually ADV RB\n",
      "developed VERB VBN\n",
      ") PUNCT -RRB-\n",
      ". PUNCT .\n",
      "When ADV WRB\n",
      "I PRON PRP\n",
      "arrived VERB VBD\n",
      "in ADP IN\n",
      "the DET DT\n",
      "gallery NOUN NN\n",
      "I PRON PRP\n",
      "could VERB MD\n",
      "not PART RB\n",
      "tell VERB VB\n",
      "where ADV WRB\n",
      "to PART TO\n",
      "start VERB VB\n",
      ", PUNCT ,\n",
      "and CCONJ CC\n",
      "security NOUN NN\n",
      "was AUX VBD\n",
      "certainly ADV RB\n",
      "not PART RB\n",
      "helpful ADJ JJ\n",
      ". PUNCT .\n",
      "I PRON PRP\n",
      "was AUX VBD\n",
      "told VERB VBN\n",
      "to PART TO\n",
      "\" PUNCT ``\n",
      "just ADV RB\n",
      "look VERB VB\n",
      "around ADV RB\n",
      "\" PUNCT ''\n",
      "and CCONJ CC\n",
      "\" PUNCT ``\n",
      "do AUX VB\n",
      "whatever DET WDT\n",
      ". PUNCT .\n",
      "\" PUNCT ''\n",
      "\n",
      "\n",
      " SPACE _SP\n",
      "At ADP IN\n",
      "such DET PDT\n",
      "a DET DT\n",
      "* PUNCT NFP\n",
      "fine ADJ JJ\n",
      "* PUNCT NFP\n",
      "institution NOUN NN\n",
      ", PUNCT ,\n",
      "I PRON PRP\n",
      "find VERB VBP\n",
      "the DET DT\n",
      "lack NOUN NN\n",
      "of ADP IN\n",
      "knowledge NOUN NN\n",
      "and CCONJ CC\n",
      "respect NOUN NN\n",
      "for ADP IN\n",
      "the DET DT\n",
      "art NOUN NN\n",
      "appalling ADJ JJ\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/alicezheng/feature-engineering-book/blob/master/03.02_Chunking_and_POS_Tagging.ipynb 참고 \n",
    "\n",
    "for doc in doc_df[0]:\n",
    "    print(doc.text, doc.pos_, doc.tag_) #.pos_: simple part-of-speech tag \n",
    "                                        #.tag_: detailed part-of-speech tag\n",
    "                                        #POS 태그 의미: https://dbrang.tistory.com/1245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[someone, who, many museums, I, this gallery, my most recent trip, Las Vegas, I, they, infamous eggs, the House, Faberge, the Virginia Museum, Fine Arts, VMFA, I, I, the gelateria, the garden, the Gallery, view, It, what, real estate agents, you, wonderful art, a gallery, any size, *s, you, me, you, * pricing, a Las Vegas attraction, the top, the space, the amount, art, you, it, it, them, the security, the show, the curating and design teams, exhibitions, a definite flow, visitors, the art, a certain sequence, it, historical period, cultural significance, audio guides, I, the gallery, I, security, I, such a *fine* institution, I, the lack, knowledge, respect]\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/alicezheng/feature-engineering-book/blob/master/03.02_Chunking_and_POS_Tagging.ipynb 참고\n",
    "\n",
    "print([chunk for chunk in doc_df[0].noun_chunks]) #noun chunking "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
